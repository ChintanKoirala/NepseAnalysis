


# this code calculates 9 days average vol  and 9 day moving average 
try:
    from nepse_scraper import Nepse_scraper
except ModuleNotFoundError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "nepse-scraper"])
    from nepse_scraper import Nepse_scraper

# -------------------- Imports --------------------
import pandas as pd
import requests
import re
from datetime import datetime

# -------------------- Config --------------------
COLUMNS = ['Symbol', 'Date', 'Open', 'Close', 'Volume']
REPO_URL = "https://api.github.com/repos/ChintanKoirala/NepseAnalysis/contents/daily_data"
RAW_BASE = "https://raw.githubusercontent.com/ChintanKoirala/NepseAnalysis/main/daily_data"
MAX_DAYS = 60

# -------------------- Find Latest combined_nepse File --------------------
def get_latest_combined_url():
    try:
        resp = requests.get(REPO_URL)
        resp.raise_for_status()
        files = resp.json()
        combined_files = [
            f["name"] for f in files if f["name"].startswith("combined_nepse_") and f["name"].endswith(".csv")
        ]
        if not combined_files:
            raise ValueError("No combined_nepse_*.csv file found in repo")

        dates = []
        for fname in combined_files:
            match = re.search(r"combined_nepse_(\d{4}-\d{2}-\d{2})\.csv", fname)
            if match:
                dates.append((match.group(1), fname))
        latest_date, latest_file = max(dates, key=lambda x: x[0])
        print(f"üìÇ Latest GitHub file found: {latest_file}")
        return f"{RAW_BASE}/{latest_file}"
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to fetch latest combined file: {e}")
        return None

LATEST_URL = get_latest_combined_url()

# -------------------- Fetch Today's NEPSE Data --------------------
scraper = Nepse_scraper()
try:
    today_data = scraper.get_today_price()
    content = today_data.get('content', [])
except Exception as e:
    print(f"‚ö†Ô∏è Failed to fetch today's NEPSE data: {e}")
    content = []

# -------------------- Process Today's Data --------------------
filtered_data = []
for item in content:
    filtered_data.append({
        'Symbol': item.get('symbol', ''),
        'Date': item.get('businessDate', ''),
        'Open': item.get('openPrice', 0),
        'Close': item.get('closePrice', 0),
        'Volume': item.get('totalTradedQuantity', 0)
    })

df_today = pd.DataFrame(filtered_data, columns=COLUMNS)

# -------------------- Save Today's File --------------------
if not df_today.empty:
    today_date = datetime.now().strftime('%Y-%m-%d')
    today_file = f"nepse_{today_date}.csv"
    df_today.to_csv(today_file, index=False)
    print(f"‚úÖ Today's data saved as '{today_file}'")
else:
    print("‚ö†Ô∏è No data available for today.")

# -------------------- Merge with Latest GitHub CSV --------------------
if not df_today.empty and LATEST_URL:
    try:
        df_latest = pd.read_csv(LATEST_URL)
        df_latest = df_latest[[col for col in COLUMNS if col in df_latest.columns]]

        df_combined = pd.concat([df_latest, df_today], ignore_index=True)
        df_combined.drop_duplicates(subset=['Symbol', 'Date'], keep='last', inplace=True)
        df_combined['Date'] = pd.to_datetime(df_combined['Date'], errors='coerce')
        df_combined.sort_values(by=['Symbol', 'Date'], inplace=True)

        N = 14  # RSI period
        output_rows = []

        for symbol, group in df_combined.groupby('Symbol'):
            group = group.copy()
            group.sort_values(by='Date', inplace=True)

            closes = pd.to_numeric(group['Close'], errors='coerce').dropna()
            if len(closes) < N + 3:
                continue

            delta = closes.diff()
            gains = delta.clip(lower=0)
            losses = -delta.clip(upper=0)

            rsi_values = []

            # Calculate RSI for last 3 trading days using standard average method
            for offset in [0, 1, 2]:  # 0 = last, 1 = 1 day before, 2 = 2 days before
                if len(closes) < N + 1 + offset:
                    rsi_values.append(None)
                    continue
                gains_period = gains.iloc[-(N + offset):-offset if offset != 0 else None]
                losses_period = losses.iloc[-(N + offset):-offset if offset != 0 else None]

                avg_gain = gains_period.mean()
                avg_loss = losses_period.mean()

                if avg_loss == 0 and avg_gain == 0:
                    rsi = 50.0
                elif avg_loss == 0:
                    rsi = 100.0
                else:
                    rs = avg_gain / avg_loss
                    rsi = 100 - (100 / (1 + rs))
                rsi_values.append(round(rsi, 2))

            last_row = group.iloc[[-1]].copy()
            last_row['Avg_Vol_9D'] = group['Volume'].rolling(window=N).mean().iloc[-1]
            last_row['MA_3D'] = group['Close'].rolling(window=3).mean().iloc[-1]
            last_row['MA_9D'] = group['Close'].rolling(window=N).mean().iloc[-1]

            # Add RSI columns
            last_row['Rsi_14D_Last'] = rsi_values[0]
            last_row['Rsi_14D_1D_Before'] = rsi_values[1]
            last_row['Rsi_14D_2D_Before'] = rsi_values[2]

            output_rows.append(last_row)

        if output_rows:
            df_final = pd.concat(output_rows, ignore_index=True)
        else:
            df_final = pd.DataFrame(columns=['Symbol', 'Date', 'Open', 'Close', 'Volume',
                                             'Avg_Vol_9D', 'MA_3D', 'MA_9D',
                                             'Rsi_14D_Last', 'Rsi_14D_1D_Before', 'Rsi_14D_2D_Before'])

        df_final['Date'] = pd.to_datetime(df_final['Date']).dt.strftime('%Y-%m-%d')
        df_final['Avg_Vol_9D'] = df_final['Avg_Vol_9D'].fillna(0).astype(int)
        df_final['MA_3D'] = df_final['MA_3D'].round(2)
        df_final['MA_9D'] = df_final['MA_9D'].round(2)

        df_final = df_final[['Symbol', 'Date', 'Open', 'Close', 'Volume',
                             'Avg_Vol_9D', 'MA_3D', 'MA_9D',
                             'Rsi_14D_Last', 'Rsi_14D_1D_Before', 'Rsi_14D_2D_Before']]

        df_final.sort_values(by='Symbol', inplace=True)
        df_final.reset_index(drop=True, inplace=True)
        df_final.index += 1
        df_final.index.name = 'S.N.'

        df_final.to_csv("completedata.csv", index=True)
        print("‚úÖ File 'completedata.csv' saved successfully with standard RSI(14D) for last 3 days.")

    except Exception as e:
        print(f"‚ö†Ô∏è Failed to process and calculate: {e}")

# upload in github


import os
import sys
import base64
import requests
from datetime import datetime
import shutil

# -------------------- GitHub Config --------------------
REPO = "ChintanKoirala/NepseAnalysis"
BRANCH = "main"
UPLOAD_FOLDER = "daily_data"
LOCAL_FILE = "completedata.csv"  # Updated output file

# -------------------- Verify Local File --------------------
if not os.path.exists(LOCAL_FILE):
    print(f"‚ùå Output file '{LOCAL_FILE}' not found. Please run the NEPSE data script first.")
    sys.exit(1)

# Get today's date
today_date = datetime.now().strftime("%Y-%m-%d")
dated_filename = f"completedata_{today_date}.csv"

# Create a dated copy to preserve original
shutil.copy(LOCAL_FILE, dated_filename)
print(f"‚úÖ Copied local file to '{dated_filename}' for upload.")

# -------------------- GitHub Token --------------------
token = os.getenv("GITHUB_TOKEN") or os.getenv("GH_PAT")
if not token:
    print("‚ùå GitHub token not found. Please set GITHUB_TOKEN or GH_PAT.")
    sys.exit(1)
else:
    print("‚úÖ GitHub token loaded successfully.")

headers = {
    "Authorization": f"Bearer {token}",
    "Accept": "application/vnd.github.v3+json"
}

# -------------------- Prepare File for Upload --------------------
repo_path = f"{UPLOAD_FOLDER}/{dated_filename}"
upload_url = f"https://api.github.com/repos/{REPO}/contents/{repo_path}"

try:
    with open(dated_filename, "rb") as f:
        content = f.read()
    encoded_content = base64.b64encode(content).decode()
    print(f"‚ÑπÔ∏è File '{dated_filename}' read and encoded successfully.")
except Exception as e:
    print(f"‚ùå Failed to read '{dated_filename}': {e}")
    sys.exit(1)

# -------------------- Check if File Already Exists --------------------
sha = None
try:
    check_resp = requests.get(upload_url, headers=headers)
    if check_resp.status_code == 200:
        sha = check_resp.json().get("sha")
        print(f"‚ÑπÔ∏è File '{repo_path}' already exists in repo. It will be updated.")
    elif check_resp.status_code == 404:
        print(f"‚ÑπÔ∏è File '{repo_path}' does not exist. A new file will be created.")
    else:
        print(f"‚ö†Ô∏è Unexpected response ({check_resp.status_code}) while checking file.")
        print(check_resp.text)
except Exception as e:
    print(f"‚ö†Ô∏è Could not check file existence: {e}")

# -------------------- Upload to GitHub --------------------
payload = {
    "message": f"Upload completedata file for {today_date}",
    "content": encoded_content,
    "branch": BRANCH
}
if sha:
    payload["sha"] = sha

try:
    upload_resp = requests.put(upload_url, headers=headers, json=payload)
    if upload_resp.status_code in [200, 201]:
        print(f"‚úÖ Successfully uploaded '{repo_path}' to GitHub repository.")
    else:
        print(f"‚ùå Upload failed! Status: {upload_resp.status_code}")
        print(upload_resp.text)
except Exception as e:
    print(f"‚ùå Exception during upload: {e}")


